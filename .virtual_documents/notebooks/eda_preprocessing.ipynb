


import warnings
warnings.filterwarnings("ignore")
import os
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud,STOPWORDS
from bs4 import BeautifulSoup
import re,string,unicodedata

from pathlib import Path





notebook_dir = Path().resolve()
csv_path = notebook_dir.parent / "data" / "IMDB_Dataset.csv"

data = pd.read_csv(csv_path)





data.head()


data.describe(include="all").T


data.info()


null_values = data.isnull().sum()
print(f"There are {null_values[0]} missing values for {null_values.index[0]} and {null_values[1]} for {null_values.index[1]}.")


num_duplicates = data.duplicated().sum() #identify duplicates
print(f"There are {num_duplicates} duplicate reviews present in the dataset")


review_data = data['review']
duplicated_review = data[review_data.isin(review_data[review_data.duplicated()])].sort_values("review")
duplicated_review.head()


data.drop_duplicates(inplace = True)
print('The dataset contains {} rows and {} columns after removing duplicates'.format(data.shape[0],data.shape[1]))




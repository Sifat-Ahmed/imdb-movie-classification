from pathlib import Path
import sys, math
import gc
import pandas as pd
import numpy as np
from time import perf_counter
from tqdm.notebook import tqdm
notebook_dir = Path().resolve()
sys.path.append(str(notebook_dir.parent / "src"))
import matplotlib.pyplot as plt
import seaborn as sns

from preprocessor.preprocessing import IMDBPreprocessor
from models.naive_bayes import NaiveBayes
from models.knn import KNN
from vectorizer.bag_of_words import BoWVectorizer  
from vectorizer.tfidf import TfidfVectorizerScratch
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split

project_root = notebook_dir.parent


from IPython.display import HTML
import matplotlib.pyplot as plt

plt.rcParams["figure.figsize"] = (20, 6)

HTML("""
<style>
.container { width:100% !important; }
.output_png { display: block; margin-left: auto; margin-right: auto; }
</style>
""")


df = pd.read_parquet(project_root / "data" / "imdb_reviews.parquet")
df = df[["review", "sentiment"]].dropna().reset_index(drop=True)


from sklearn.model_selection import train_test_split
X_train_text, X_test_text, y_train, y_test = train_test_split(
    df["review"].astype(str), df["sentiment"].astype(int), test_size=0.2, random_state=42, stratify=df["sentiment"]
)


X_train_tok = X_train_text.str.split().tolist()
X_test_tok  = X_test_text.str.split().tolist()





vectorizers = {
    "bow": lambda: BoWVectorizer(binary=False, min_df=2),
    "bow_binary": lambda: BoWVectorizer(binary=True, min_df=2),
    "tfidf": lambda: TfidfVectorizerScratch(min_df=2, sublinear_tf=False, l2_norm=False),
    "tfidf_norm": lambda: TfidfVectorizerScratch(min_df=2, sublinear_tf=True, l2_norm=True),
}

models = {
    "naive_bayes": lambda: NaiveBayes(alpha=1.0),
    "knn": lambda: KNN(k=7, distance_metric="cosine"),
}


results = []  # store only y_true & y_pred for each combination

v_items = list(vectorizers.items())
m_items = list(models.items())

for v_name, v_ctor in tqdm(v_items, desc="Vectorizers", total=len(v_items)):
    vec = v_ctor()
    X_tr = vec.fit_transform(X_train_tok)
    X_te = vec.transform(X_test_tok)

    for m_name, m_ctor in tqdm(m_items, desc=f"Models ({v_name})", total=len(m_items), leave=False):
        model = m_ctor()
        model.fit(X_tr, y_train.tolist())
        y_pred = model.predict(X_te)

        results.append({
            "vectorizer": v_name,
            "model": m_name,
            "y_true": y_test.to_numpy() if hasattr(y_test, "to_numpy") else np.asarray(y_test),
            "y_pred": np.asarray(y_pred),
        })

        del model, y_pred
        gc.collect()

    del X_tr, X_te, vec
    gc.collect()



summary = []
for r in results:
    acc = accuracy_score(r["y_true"], r["y_pred"])
    f1  = f1_score(r["y_true"], r["y_pred"])
    summary.append({"vectorizer": r["vectorizer"], "model": r["model"], "accuracy": acc, "f1": f1})

results_df = pd.DataFrame(summary).sort_values(["accuracy", "f1"], ascending=False).reset_index(drop=True)
results_df


best_row = results_df.iloc[0]
best_vec_name = best_row["vectorizer"]
best_model_name = best_row["model"]
print(f"Best combo: {best_vec_name} + {best_model_name} | acc={best_row['accuracy']:.4f} f1={best_row['f1']:.4f}")


for r in results:
    print("="*60)
    print(f"Vectorizer: {r['vectorizer']} | Model: {r['model']}")
    print(classification_report(r["y_true"], r["y_pred"], target_names=["neg","pos"]))


def plot_cm_grid(results, n_rows=2, n_cols=4, cmap="Blues"):
    total = len(results)
    per_fig = n_rows * n_cols
    num_figs = math.ceil(total / per_fig)

    # global max for consistent color scaling
    global_max = max(
        confusion_matrix(r["y_true"], r["y_pred"], labels=[0, 1]).max()
        for r in results
    )

    for f in range(num_figs):
        sub = results[f*per_fig : (f+1)*per_fig]
        fig, axes = plt.subplots(n_rows, n_cols)
        axes = np.atleast_1d(axes).ravel()

        for ax, r in zip(axes, sub):
            cm = confusion_matrix(r["y_true"], r["y_pred"], labels=[0, 1])
            hm = sns.heatmap(
                cm, annot=False, cmap=cmap, cbar=False, square=True,
                xticklabels=["neg","pos"], yticklabels=["neg","pos"], ax=ax,
                vmin=0, vmax=global_max  # global intensity
            )
            ax.set_title(f"{r['vectorizer']} + {r['model']}")
            ax.set_xlabel("Predicted"); ax.set_ylabel("True")

            # contrast-aware annotations
            for i in range(cm.shape[0]):
                for j in range(cm.shape[1]):
                    val = cm[i, j]
                    frac = val / global_max if global_max else 0.0
                    color = "white" if frac > 0.5 else "black"
                    ax.text(j + 0.5, i + 0.5, f"{val}",
                            ha="center", va="center",
                            color=color, fontsize=10)

        # turn off unused axes
        for ax in axes[len(sub):]:
            ax.axis("off")

        plt.tight_layout()
        plt.show()

plot_cm_grid(results, n_rows=2, n_cols=4)


def get_best_worst(results):
    scored = [(accuracy_score(r["y_true"], r["y_pred"]), r) for r in results]
    scored.sort(key=lambda x: x[0], reverse=True)
    return scored[0][1], scored[-1][1]  # best, worst

best, worst = get_best_worst(results)

def rebuild_and_get_misclassified(v_name, m_name, X_train_tok, y_train, X_test_tok, y_test, raw_test_texts):
    # rebuild vectorizer
    vec = vectorizers[v_name]()
    X_tr = vec.fit_transform(X_train_tok)
    X_te = vec.transform(X_test_tok)

    # rebuild and predict
    model = models[m_name]()
    model.fit(X_tr, y_train.tolist())
    y_pred = model.predict(X_te)

    # find misclassified
    mis_idx = np.where(y_pred != y_test)[0]
    return [(raw_test_texts[i], y_test[i], y_pred[i]) for i in mis_idx]

# Get misclassifications
mis_best = rebuild_and_get_misclassified(best["vectorizer"], best["model"],
                                         X_train_tok, y_train, X_test_tok, y_test, X_test_text)

mis_worst = rebuild_and_get_misclassified(worst["vectorizer"], worst["model"],
                                          X_train_tok, y_train, X_test_tok, y_test, X_test_text)

# Show first 5 for each
print("Best Model Misclassifications:")
for text, true, pred in mis_best[:5]:
    print(f"True: {true}, Pred: {pred}\n{text}\n")

print("\nWorst Model Misclassifications:")
for text, true, pred in mis_worst[:5]:
    print(f"True: {true}, Pred: {pred}\n{text}\n")


X_test_raw = X_test_text.reset_index(drop=True).astype(str).tolist()
y_true_arr = y_test.reset_index(drop=True).to_numpy()

def rebuild_predict(v_name, m_name):
    """Rebuild vectorizer & model from scratch, return predictions."""
    vec = vectorizers[v_name]()
    X_tr = vec.fit_transform(X_train_tok)
    X_te = vec.transform(X_test_tok)
    model = models[m_name]()
    model.fit(X_tr, y_train.tolist())
    return model.predict(X_te)

# --- Get best and worst classifiers ---
# assuming you already have results_df sorted best â†’ worst
best  = results_df.iloc[0]
worst = results_df.iloc[-1]

# --- Predict for best & worst ---
y_pred_best  = rebuild_predict(best["vectorizer"],  best["model"])
y_pred_worst = rebuild_predict(worst["vectorizer"], worst["model"])

# --- Misclassified lists ---
mis_best  = [(X_test_raw[i], y_true_arr[i], y_pred_best[i])  
             for i in np.where(y_pred_best  != y_true_arr)[0]]

mis_worst = [(X_test_raw[i], y_true_arr[i], y_pred_worst[i]) 
             for i in np.where(y_pred_worst != y_true_arr)[0]]

# --- Compare predictions ---
cmp_df = pd.DataFrame({
    "text": X_test_raw,
    "true": y_true_arr,
    "pred_best": y_pred_best,
    "pred_worst": y_pred_worst,
})

# Best got it right, worst got it wrong
wrong_worst_right_best = cmp_df[
    (cmp_df["pred_best"] == cmp_df["true"]) & 
    (cmp_df["pred_worst"] != cmp_df["true"])
]

# Worst got it right, best got it wrong
right_worst_wrong_best = cmp_df[
    (cmp_df["pred_best"] != cmp_df["true"]) & 
    (cmp_df["pred_worst"] == cmp_df["true"])
]

# --- Show examples ---
print("\n=== Best Correct / Worst Wrong ===")
print(wrong_worst_right_best.head(10)[["true","pred_best","pred_worst","text"]])

print("\n=== Worst Correct / Best Wrong ===")
print(right_worst_wrong_best.head(10)[["true","pred_best","pred_worst","text"]])







import warnings
warnings.filterwarnings("ignore")
import os, sys
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud,STOPWORDS
from bs4 import BeautifulSoup
import re,string,unicodedata
from collections import Counter
from pathlib import Path


from IPython.display import HTML
import matplotlib.pyplot as plt

plt.rcParams["figure.figsize"] = (20, 6)

HTML("""
<style>
.container { width:100% !important; }
.output_png { display: block; margin-left: auto; margin-right: auto; }
</style>
""")





notebook_dir = Path().resolve()
csv_path = notebook_dir.parent / "data" / "IMDB_Dataset.csv"

data = pd.read_csv(csv_path)





data.head()


data.describe(include="all").T


data.info()


null_values = data.isnull().sum()
print(f"There are {null_values[0]} missing values for {null_values.index[0]} and {null_values[1]} for {null_values.index[1]}.")


num_duplicates = data.duplicated().sum() #identify duplicates
print(f"There are {num_duplicates} duplicate reviews present in the dataset")


review_data = data['review']
duplicated_review = data[review_data.isin(review_data[review_data.duplicated()])].sort_values("review")
duplicated_review.head()


data.drop_duplicates(inplace = True)
print('The dataset contains {} rows and {} columns after removing duplicates'.format(data.shape[0],data.shape[1]))


sys.path.append(str(notebook_dir.parent / "src"))
from preprocessor.preprocessing import IMDBPreprocessor


preprocessor = IMDBPreprocessor()


backup_data = data.copy()

data["review"] = data["review"].apply(preprocessor.preprocess)


data["sentiment"] = [1 if sentiment == "positive" else 0 for sentiment in data["sentiment"]]


data.head()


sns.set(style = "whitegrid" , font_scale = 1.2)
sns.countplot(data, x="sentiment", palette = ['green','red'],order = [1,0])
plt.xticks(ticks = np.arange(2),labels = ['positive','negative'])
plt.title('Positive Negative Review count Plot')
plt.show()


print(f"Positive reviews are {round(data['sentiment'].value_counts()[0])} i.e. {round(data['sentiment'].value_counts()[0]/len(data) * 100,2)} % of the dataset")
print(f"Negative reviews are {round(data['sentiment'].value_counts()[1])} i.e. {round(data['sentiment'].value_counts()[1]/len(data) * 100,2)} % of the dataset")


data["review_word_count"] = data["review"].str.split().str.len()
data["review_char_count"] = data["review"].str.len()


#plt.figure(figsize=(14, 6))
sns.histplot(
    data=data,
    x="review_word_count",
    hue="sentiment",
    bins=50,
    palette={1: "green", 0: "red"},
    alpha=0.6
)
plt.title("Review Word Count Distribution by Sentiment")
plt.xlabel("Number of Words")
plt.ylabel("Number of Reviews")
plt.xlim(0, data["review_word_count"].quantile(0.95))
plt.show()


sns.histplot(
    data=data,
    x="review_char_count",
    hue="sentiment",
    bins=50,
    palette={1: "green", 0: "red"},
    alpha=0.6
)
plt.title("Review Character Count Distribution by Sentiment")
plt.xlabel("Number of Characters")
plt.ylabel("Number of Reviews")
plt.xlim(0, data["review_char_count"].quantile(0.95))
plt.show()


all_words = " ".join(data["review"]).split()
word_freq = Counter(all_words)
common_words_df = pd.DataFrame(word_freq.most_common(20), columns=["word", "count"])


#plt.figure(figsize=(12,6))
sns.barplot(data=common_words_df, x="count", y="word", palette="viridis")
plt.title("Top 20 Most Common Words in Reviews")
plt.xlabel("Count")
plt.ylabel("Word")
plt.show()


# Positive reviews
pos_words = " ".join(data.loc[data["sentiment"] == 1, "review"]).split()
pos_freq = pd.DataFrame(Counter(pos_words).most_common(20), columns=["word", "count"])
pos_freq["sentiment"] = "Positive"

# Negative reviews
neg_words = " ".join(data.loc[data["sentiment"] == 0, "review"]).split()
neg_freq = pd.DataFrame(Counter(neg_words).most_common(20), columns=["word", "count"])
neg_freq["sentiment"] = "Negative"

# Combine
freq_df = pd.concat([pos_freq, neg_freq])


#plt.figure(figsize=(14,8))
sns.barplot(data=freq_df, x="count", y="word", hue="sentiment", palette={"Positive":"green", "Negative":"red"})
plt.title("Top 20 Words per Sentiment")
plt.xlabel("Count")
plt.ylabel("Word")
plt.show()


from sklearn.feature_extraction.text import CountVectorizer

def get_top_bigrams(texts, top_n=20):
    vectorizer = CountVectorizer(ngram_range=(2,2), max_features=top_n)
    X = vectorizer.fit_transform(texts)
    bigram_counts = X.toarray().sum(axis=0)
    return pd.DataFrame({
        "bigram": vectorizer.get_feature_names_out(),
        "count": bigram_counts
    }).sort_values(by="count", ascending=False)

# Positive reviews bigrams
pos_bigrams_df = get_top_bigrams(data.loc[data["sentiment"] == 1, "review"])
pos_bigrams_df["sentiment"] = "Positive"

# Negative reviews bigrams
neg_bigrams_df = get_top_bigrams(data.loc[data["sentiment"] == 0, "review"])
neg_bigrams_df["sentiment"] = "Negative"


fig, axes = plt.subplots(1, 2, figsize=(20, 8), sharex=False, sharey=False)

sns.barplot(data=pos_bigrams_df, x="count", y="bigram", palette="Greens_r", ax=axes[0])
axes[0].set_title("Top 20 Positive Bigrams")
axes[0].set_xlabel("Count")
axes[0].set_ylabel("Bigram")

sns.barplot(data=neg_bigrams_df, x="count", y="bigram", palette="Reds_r", ax=axes[1])
axes[1].set_title("Top 20 Negative Bigrams")
axes[1].set_xlabel("Count")
axes[1].set_ylabel("")

plt.tight_layout()
plt.show()


extra_stops = {"movie", "film", "one", "really", "make", "made"}
stops = STOPWORDS.union(extra_stops)

pos_text = " ".join(data.loc[data["sentiment"] == 1, "review"].astype(str))
neg_text = " ".join(data.loc[data["sentiment"] == 0, "review"].astype(str))

wc_pos = WordCloud(
    width=1600, height=900, background_color="white",
    stopwords=stops, collocations=False, random_state=42
).generate(pos_text)

wc_neg = WordCloud(
    width=1600, height=900, background_color="white",
    stopwords=stops, collocations=False, random_state=42
).generate(neg_text)

plt.figure(figsize=(18, 8))
plt.subplot(1, 2, 1)
plt.imshow(wc_pos, interpolation="bilinear")
plt.axis("off")
plt.title("Positive Reviews", fontsize=16)

plt.subplot(1, 2, 2)
plt.imshow(wc_neg.recolor(color_func=lambda *args, **kw: "red"), interpolation="bilinear")
plt.axis("off")
plt.title("Negative Reviews", fontsize=16)

plt.tight_layout()
plt.show()





from preprocessor.pos_tag import compute_pos_stats

texts = data["review"].tolist()
labels = data["sentiment"].tolist()

pos_df = compute_pos_stats(texts, labels)


top_k = 12
order = (
    pos_df.groupby("pos")["count"]
    .sum()
    .sort_values(ascending=False)
    .head(top_k)
    .index.tolist()
)

#plt.figure(figsize=(12, 6))
sns.barplot(
    data=pos_df[pos_df["pos"].isin(order)],
    x="pos",
    y="pct",
    hue="sentiment",
    order=order,
    palette={1: "green", 0: "red"},
)
plt.title("Top POS tags by relative frequency (NLTK)")
plt.xlabel("POS")
plt.ylabel("Share")
plt.legend(title="Sentiment", labels=["Positive (G)", "Negative (R)"])
plt.tight_layout()
plt.show()



